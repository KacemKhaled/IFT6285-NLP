{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors, Word2Vec\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.test.utils import datapath\n",
    "import argparse\n",
    "import multiprocessing\n",
    "from time import time\n",
    "from os import listdir\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import logging\n",
    "import numpy as np\n",
    "from gensim.models.word2vec import  PathLineSentences, LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder = 'training-monolingual.tokenized.shuffled/'\n",
    "short_folder = 'training-monolingual.tokenized.shuffled_short/'\n",
    "words_list_file= 'liste_mots_devoir4.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"train a gensim word2vec model from text read on stdin\")\n",
    "\n",
    "    parser.add_argument(\"-v\", '--verbosity', type=int, help=\"increase output verbosity\", default=0)\n",
    "    parser.add_argument(\"-n\", '--nb', type=int, help=\"# of input lines to read\", default=None)\n",
    "    parser.add_argument(\"-d\", '--datapath', type=str, help=\"directory where txt files can be found\", default=None)\n",
    "    parser.add_argument(\"-f\", '--name', type=str, help=\"basename modelname\", default=\"genw2v\")\n",
    "    parser.add_argument(\"-s\", '--size', type=int, help=\"dim of vectors\", default=300)\n",
    "    parser.add_argument(\"-g\", '--negative', type=int, help=\"# neg samples\", default=100)\n",
    "    parser.add_argument(\"-c\", '--mincount', type=int, help=\"min count of a word\", default=1)\n",
    "    parser.add_argument(\"-w\", '--window', type=int, help=\"window size\", default=2)\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "def plot(ext,times, sent_len):\n",
    "\n",
    "    print('Creating the figure')\n",
    "    \n",
    "\n",
    "    plt.figure(figsize=(9, 6))\n",
    "    plt.plot(sent_len, times, 'r--')\n",
    "    plt.title(\"Le temps mis pour entrainer les modeles en fonction du nombre de phrases considérées\")\n",
    "    plt.xlabel(\"Nombre de phrases considérées\")\n",
    "    plt.ylabel(\"Le temps mis pour entrainer les modeles (en secondes)\")\n",
    "    plt.savefig(f\"courbe-{ext}.svg\",format=\"svg\")\n",
    "    plt.savefig(f\"courbe-{ext}.png\", format=\"png\")\n",
    "    plt.savefig(f\"courbe-{ext}.eps\", format=\"eps\")\n",
    "\n",
    "\n",
    "def similar_words(file,modelname):\n",
    "    model = Word2Vec.load(modelname)\n",
    "    unrecognized_words = []\n",
    "    with open(file, 'r', encoding=\"utf8\") as in_file, \\\n",
    "     open('out_'+file, mode='w') as out_file:\n",
    "        lines = in_file.readlines()\n",
    "        print(f\"Looking for similar words for {len(lines)} words\")\n",
    "        for line in tqdm(lines):\n",
    "            word = line.strip()\n",
    "            try:\n",
    "                liste = model.wv.most_similar(positive=[word])[:10]\n",
    "                line_out = \"\"\n",
    "                for x in liste:\n",
    "                    line_out+= f\"{x[0]} [{x[1]:.2f}] \" \n",
    "                out_file.write(f\"{word}\\t{line_out}\\n\")\n",
    "            except:\n",
    "                unrecognized_words.append(word)\n",
    "                print(f\"The word: '{word}' is not in vocab\")\n",
    "    np.savetxt('unrecognized_words.txt', unrecognized_words ,delimiter =\"\\n\", fmt ='% s')    \n",
    "    in_file.close()\n",
    "    out_file.close()\n",
    "    \n",
    "def checkpoint(ext,save_model,w2v_model,times,sizes):\n",
    "    \n",
    "    modelname = f\"models/{ext}.w2v\"\n",
    "    #txt_modelname  = f\"outputs/{ext}.txt\"\n",
    "    csv_times_sizes= f\"outputs/{ext}.csv\"\n",
    "\n",
    "    if save_model:\n",
    "        # stream the model\n",
    "        w2v_model.init_sims(replace=True)\n",
    "        w2v_model.save(modelname)\n",
    "        logging.info(f\"saved {modelname}\")\n",
    "\n",
    "    # this one exports a textfile that spacy can (hopefully) convert\n",
    "    \n",
    "    # w2v_model.wv.save_word2vec_format(txt_modelname)\n",
    "    # logging.info(f\"saved {txt_modelname}\")\n",
    "    \n",
    "    np.savetxt(csv_times_sizes, [times,sizes],delimiter =\", \", fmt ='% s')\n",
    "    logging.info(f\"saved {csv_times_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(args,  save_model = True , data = folder,nb_tranches=10):\n",
    "    # Utilisez gensim pour entrainer des representations vectorielles sur tout ou partie du 1BWC.\n",
    "    #train by tranche + save the time for each tranche\n",
    "    print('Starting the training 1')\n",
    "    files = listdir(data)[:nb_tranches]\n",
    "    sents = []\n",
    "    times = []\n",
    "    times_v = []\n",
    "    sizes = []\n",
    "    sent_len = []\n",
    "\n",
    "    # Time to build model \n",
    "    start_time = time()\n",
    "    w2v_model = Word2Vec(\n",
    "        #sentences = sents,#LineSentence(datapath(training_sample)),\n",
    "        #corpus_file=training_sample,\n",
    "        min_count=args.mincount, window=args.window,  alpha=1e-2, vector_size=args.size,\n",
    "        min_alpha=1e-4, workers=(os.cpu_count()*2 - 1), sample=0.01, negative=args.negative\n",
    "        )\n",
    "    end_m = round((time() - start_time),2)\n",
    "    print(f\"- Temps de construction de modèle (en secondes): {end_m}\\n\")\n",
    "\n",
    "    for i,fn in enumerate(files):\n",
    "        print(f\"\\n{'#-'*10}-Files 1..{i+1}:\")\n",
    "\n",
    "        start_time = time()\n",
    "        sents = LineSentence(data+fn)\n",
    "        print(f\"- Temps de lecture de sentences (en secondes): {(time() - start_time)}\\n\")\n",
    "\n",
    "        start_time = time()\n",
    "        w2v_model.build_vocab(\n",
    "            corpus_iterable=sents,#LineSentence(datapath(training_sample)),\n",
    "            #corpus_file=training_sample, \n",
    "            progress_per=10000, update=True if i>0 else False\n",
    "            )\n",
    "        end_v = round((time() - start_time)/60,2)\n",
    "        times_v.append(end_v)\n",
    "        print(f\"- Temps de construction de vocabulaire (en minutes): {end_v}\\n\")\n",
    "\n",
    "        # Time to train the model\n",
    "        start_time = time()\n",
    "        w2v_model.train( corpus_iterable = sents,#LineSentence(datapath(training_sample)),\n",
    "            #corpus_file=training_sample,\n",
    "            total_examples=w2v_model.corpus_count, epochs=10, report_delay=1\n",
    "                        ) \n",
    "        end = round((time() - start_time)/60,2)\n",
    "        times.append(end)\n",
    "        print(f\"- Temps d'entrainement (en minutes): {end}\\n\")\n",
    "\n",
    "        # Size of the model\n",
    "        size = round(w2v_model.estimate_memory()['total']/(1024*1024),2)\n",
    "        sizes.append(size)\n",
    "        sent_len.append()\n",
    "\n",
    "        print(f\"- Taille du modele sur disque (en octets)): {w2v_model.estimate_memory()}\\n\\\n",
    "                Total en MB: {size}\")\n",
    "\n",
    "        print(\"\\n- Nombre de mots encodés (= taille du vocab): %d\\n\" % len(w2v_model.wv.vectors))\n",
    "        #print(w2v_model.most_similar(positive=['abnormalities'], topn = 10))\n",
    "        #w2v_model.init_sims(replace=True)\n",
    "        print(w2v_model.wv.most_similar(positive=['abnormalities'])[:10])\n",
    "\n",
    "    ext = \"{}-size{}-window{}-neg{}-mincount{}\".format(args.name, args.size, args.window, args.negative, args.mincount)\n",
    "\n",
    "    checkpoint(ext,save_model,w2v_model,times,sizes)\n",
    "\n",
    "    plot(ext,times, sent_len)\n",
    "    similar_words(words_list_file)\n",
    "    return w2v_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train2(args,  save_model = True , data = folder,nb_tranches=10):\n",
    "    # Utilisez gensim pour entrainer des representations vectorielles sur tout ou partie du 1BWC.\n",
    "    #train by tranche + save the time for each tranche\n",
    "    print('Starting the training 2')\n",
    "    files = listdir(data)[:nb_tranches]\n",
    "    sents = []\n",
    "    times = []\n",
    "    times_v = []\n",
    "    sizes = []\n",
    "    sent_len = []\n",
    "\n",
    "    for i,fn in enumerate(files):\n",
    "        print(f\"\\n{'#-'*10}-Files 1..{i+1}:\")\n",
    "        with open(data + fn, 'r', encoding=\"utf8\") as f:\n",
    "            ## OLD ##\n",
    "            # corpus = f.read()  # TRANCHE\n",
    "            # sentences = corpus.split('\\n')  #phrases in the tranche\n",
    "            # sents.append(sentences) # train accumulative\n",
    "\n",
    "            # phrases = Phrases(sents, min_count=10, progress_per=1000)\n",
    "            # bigram = Phraser(phrases)\n",
    "            # sents_b = bigram[sents]\n",
    "            \n",
    "            ## NEW ##\n",
    "            sents.append(f.readlines())\n",
    "            #sents_b = Phraser(Phrases(sents, min_count=1, progress_per=1000))[sents]\n",
    "\n",
    "            # Time to build model with vocab\n",
    "            start_time = time()\n",
    "            w2v_model = Word2Vec(\n",
    "                sentences = sents,\n",
    "                #corpus_file=training_sample,\n",
    "                min_count=args.mincount, window=args.window,  alpha=1e-2, vector_size=args.size,\n",
    "                min_alpha=1e-4, workers=(os.cpu_count()*2 - 1), sample=0.01, negative=args.negative\n",
    "                )\n",
    "            end_m = round((time() - start_time)/60,2)\n",
    "            print(f\"- Temps de construction de modèle (en minutes): {end_m}\\n\")\n",
    "\n",
    "            start_time = time()\n",
    "            w2v_model.build_vocab(\n",
    "                corpus_iterable=sents,\n",
    "                #corpus_file=training_sample, \n",
    "                progress_per=10000,\n",
    "                #update=True\n",
    "                )\n",
    "            end_v = round((time() - start_time)/60,2)\n",
    "            times_v.append(end_v)\n",
    "            print(f\"- Temps de construction de vocabulaire (en minutes): {end_v}\\n\")\n",
    "\n",
    "            # Time to train the model\n",
    "            start_time = time()\n",
    "            w2v_model.train( corpus_iterable = sents,\n",
    "                #corpus_file=training_sample,\n",
    "                total_examples=w2v_model.corpus_count, epochs=10, report_delay=1\n",
    "                            ) \n",
    "            end = round((time() - start_time)/60,2)\n",
    "            times.append(end)\n",
    "            print(f\"- Temps d'entrainement (en minutes): {end}\\n\")\n",
    "\n",
    "            # Size of the model\n",
    "            size = round(w2v_model.estimate_memory()['total']/(1024*1024),2)\n",
    "            sizes.append(size)\n",
    "\n",
    "            print(f\"- Taille du modele sur disque (en octets)): {w2v_model.estimate_memory()}\\n\\\n",
    "                    Total en MB: {size}\")\n",
    "\n",
    "            print(\"\\n- Nombre de mots encodés (= taille du vocab): %d\\n\" % len(w2v_model.wv.vectors))\n",
    "            w2v_model.init_sims(replace=True)\n",
    "            #print(w2v_model.most_similar(positive=['abnormalities'], topn = 10))\n",
    "            print(w2v_model.wv.most_similar(positive=['abnormalities'])[:10])\n",
    "        f.close()\n",
    "    ext = \"{}-size{}-window{}-neg{}-mincount{}\".format(args.name, args.size, args.window, args.negative, args.mincount)\n",
    "\n",
    "    checkpoint(ext,save_model,w2v_model,times,sizes)\n",
    "\n",
    "    plot(ext,times, sent_len)\n",
    "    similar_words(words_list_file)\n",
    "    return w2v_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the training 2\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..1:\n",
      "- Temps de construction de modèle (en minutes): 0.31\n",
      "\n",
      "- Temps de construction de vocabulaire (en minutes): 0.09\n",
      "\n",
      "- Temps d'entrainement (en minutes): 0.95\n",
      "\n",
      "- Taille du modele sur disque (en octets)): {'vocab': 153027000, 'vectors': 367264800, 'syn1neg': 367264800, 'total': 887556600}\n",
      "                    Total en MB: 846.44\n",
      "\n",
      "- Nombre de mots encodés (= taille du vocab): 306054\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-6f01e878b1d8>:68: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v_model.init_sims(replace=True)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"Key 'abnormalities' not present\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-36c706552749>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[0mmain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-16-36c706552749>\u001b[0m in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mtrain2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfolder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnb_tranches\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# remove the data argument for all tranches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-6f01e878b1d8>\u001b[0m in \u001b[0;36mtrain2\u001b[1;34m(args, save_model, data, nb_tranches)\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[1;31m#print(w2v_model.most_similar(positive=['abnormalities'], topn = 10))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'abnormalities'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m         \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m     \u001b[0mext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"{}-size{}-window{}-neg{}-mincount{}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmincount\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    771\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    772\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 773\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    774\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_index_for\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    775\u001b[0m                     \u001b[0mall_keys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, key, norm)\u001b[0m\n\u001b[0;32m    436\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    437\u001b[0m         \"\"\"\n\u001b[1;32m--> 438\u001b[1;33m         \u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    439\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfill_norms\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_index\u001b[1;34m(self, key, default)\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mdefault\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    411\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 412\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Key '{key}' not present\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    413\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"Key 'abnormalities' not present\""
     ]
    }
   ],
   "source": [
    "def get_args_notebook():\n",
    "    class Args:\n",
    "        def __init__(self) -> None:\n",
    "            super().__init__()\n",
    "            self.name, self.size, self.window, self.negative, self.mincount = \"genw2v\",300,2,100,1\n",
    "    args = Args()\n",
    "    return args\n",
    "def main():\n",
    "    args = get_args_notebook()\n",
    "    \n",
    "    ext = \"{}-size{}-window{}-neg{}-mincount{}\".format(args.name, args.size, args.window, args.negative, args.mincount)\n",
    "\n",
    "    logname  = f\"outputs/{ext}.log\"\n",
    "\n",
    "    # logging.basicConfig(#filename=logname,\n",
    "    #                     format=\"%(levelname)s %(asctime)s  %(message)s\",\n",
    "    #                     datefmt= '%H:%M:%S', level=logging.INFO,\n",
    "    #                     handlers=[\n",
    "    #                         logging.FileHandler(logname),  logging.StreamHandler()\n",
    "    #                     ])\n",
    "    logging.basicConfig(filename=logname,datefmt= '%H:%M:%S', format='%(message)s', level=logging.DEBUG)\n",
    "\n",
    "    \n",
    "\n",
    "    train2(args, data = folder,nb_tranches=10)  # remove the data argument for all tranches\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the training 2\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..1:\n",
      "306069\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-6652a329eb6c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m     \u001b[0msents\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mLineSentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#.append(f.read().split('\\n'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[0mphrases\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprogress_per\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m     \u001b[0mbigram\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPhraser\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mphrases\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m     \u001b[0msents_b\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbigram\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[1;31m# sents_b = Phraser(Phrases(sents, min_count=10, progress_per=1000))[sents]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args = get_args_notebook()\n",
    "save_model = True \n",
    "data = folder\n",
    "nb_tranches=10\n",
    "# Utilisez gensim pour entrainer des representations vectorielles sur tout ou partie du 1BWC.\n",
    "#train by tranche + save the time for each tranche\n",
    "print('Starting the training 2')\n",
    "files = listdir(data)[:nb_tranches]\n",
    "sents = []\n",
    "times = []\n",
    "times_v = []\n",
    "sizes = []\n",
    "sent_len = []\n",
    "i,fn = 0,files[0]\n",
    "#for i,fn in enumerate(files):\n",
    "print(f\"\\n{'#-'*10}-Files 1..{i+1}:\")\n",
    "with open(data + fn, 'r', encoding=\"utf8\") as f:\n",
    "    ## OLD ##\n",
    "    corpus = f.read()  # TRANCHE\n",
    "    sentences = corpus.split('\\n')  #phrases in the tranche\n",
    "    sents.append(sentences) # train accumulative\n",
    "    print(len(sentences))\n",
    "    \n",
    "\n",
    "    # phrases = Phrases(sents, min_count=10, progress_per=1000)\n",
    "    # bigram = Phraser(phrases)\n",
    "    # sents_b = bigram[sents]\n",
    "    \n",
    "    ## NEW ##\n",
    "    \n",
    "    sents= LineSentence(f) #.append(f.read().split('\\n'))\n",
    "    phrases = Phrases(sents, min_count=10, progress_per=1000)\n",
    "    bigram = Phraser(phrases)\n",
    "    sents_b = bigram[sents]\n",
    "    # sents_b = Phraser(Phrases(sents, min_count=10, progress_per=1000))[sents]\n",
    "    print(f\"Phrases found  {len(phrases.vocab)} phrases\")\n",
    "    sent_len.append(len(phrases.vocab))\n",
    "\n",
    "    # Time to build model with vocab\n",
    "    start_time = time()\n",
    "    w2v_model = Word2Vec(\n",
    "        #sentences = sents_b,\n",
    "        #corpus_file=training_sample,\n",
    "        min_count=args.mincount, window=args.window,  alpha=0.03, vector_size=args.size,\n",
    "        min_alpha=0.0007, workers=(os.cpu_count()*2 - 1), sample=6e-5, negative=args.negative\n",
    "        )\n",
    "    end_m = round((time() - start_time)/60,2)\n",
    "    print(f\"- Temps de construction de modèle (en minutes): {end_m}\\n\")\n",
    "\n",
    "    start_time = time()\n",
    "    w2v_model.build_vocab(\n",
    "        corpus_iterable=sents_b,\n",
    "        #corpus_file=training_sample, \n",
    "        progress_per=10000,\n",
    "        #update=True\n",
    "        )\n",
    "    end_v = round((time() - start_time)/60,2)\n",
    "    times_v.append(end_v)\n",
    "    print(f\"- Temps de construction de vocabulaire (en minutes): {end_v}\\n\")\n",
    "\n",
    "    # Time to train the model\n",
    "    start_time = time()\n",
    "    w2v_model.train( corpus_iterable = sents_b,\n",
    "        #corpus_file=training_sample,\n",
    "        total_examples=w2v_model.corpus_count, epochs=10, report_delay=1\n",
    "                    ) \n",
    "    end = round((time() - start_time)/60,2)\n",
    "    times.append(end)\n",
    "    print(f\"- Temps d'entrainement (en minutes): {end}\\n\")\n",
    "\n",
    "    # Size of the model\n",
    "    size = round(w2v_model.estimate_memory()['total']/(1024*1024),2)\n",
    "    sizes.append(size)\n",
    "\n",
    "    print(f\"- Taille du modele sur disque (en octets)): {w2v_model.estimate_memory()}\\n\\\n",
    "            Total en MB: {size}\")\n",
    "\n",
    "    print(\"\\n- Nombre de mots encodés (= taille du vocab): %d\\n\" % len(w2v_model.wv.vectors))\n",
    "    w2v_model.init_sims(replace=True)\n",
    "    #print(w2v_model.most_similar(positive=['abnormalities'], topn = 10))\n",
    "    #print(w2v_model.wv.most_similar(positive=['abnormalities'])[:10])\n",
    "f.close()\n",
    "ext = \"{}-size{}-window{}-neg{}-mincount{}\".format(args.name, args.size, args.window, args.negative, args.mincount)\n",
    "\n",
    "checkpoint(ext,save_model,w2v_model,times,sizes)\n",
    "sent_len = [len(s) for s in sents]\n",
    "plot(ext,times, sent_len)\n",
    "similar_words(words_list_file,modelname=f\"models/{ext}.w2v\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 9/150 [00:00<00:01, 88.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking for similar words for 150 words\n",
      "The word: 'accrington' is not in vocab\n",
      "The word: 'adkins' is not in vocab\n",
      "The word: 'ahmadi' is not in vocab\n",
      "The word: 'ainslie' is not in vocab\n",
      "The word: 'alfredsson' is not in vocab\n",
      "The word: 'allianz' is not in vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▍      | 51/150 [00:00<00:00, 127.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word: 'amelie' is not in vocab\n",
      "The word: 'ampatuan' is not in vocab\n",
      "The word: 'anatoly' is not in vocab\n",
      "The word: 'anglicans' is not in vocab\n",
      "The word: 'angolan' is not in vocab\n",
      "The word: 'annenberg' is not in vocab\n",
      "The word: 'arturo' is not in vocab\n",
      "The word: 'asher' is not in vocab\n",
      "The word: 'ashfaq' is not in vocab\n",
      "The word: 'ashford' is not in vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 64/150 [00:00<00:00, 113.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word: 'christopher' is not in vocab\n",
      "The word: 'lebanon' is not in vocab\n",
      "The word: 'recalled' is not in vocab\n",
      "The word: 'darfur' is not in vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 59%|█████▊    | 88/150 [00:00<00:00, 105.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The word: 'sunni' is not in vocab\n",
      "The word: 'tennessee' is not in vocab\n",
      "The word: 'carlos' is not in vocab\n",
      "The word: 'brazilian' is not in vocab\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 150/150 [00:01<00:00, 93.81it/s]\n"
     ]
    }
   ],
   "source": [
    "similar_words(words_list_file,modelname=f\"models/{ext}.w2v\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the training 1\n",
      "- Temps de construction de modèle (en secondes): 0.04\n",
      "\n",
      "\n",
      "#-#-#-#-#-#-#-#-#-#--Files 1..1:\n",
      "- Temps de lecture de sentences (en secondes): 0.0\n",
      "\n",
      "- Temps de construction de vocabulaire (en minutes): 0.09\n",
      "\n",
      "- Temps d'entrainement (en minutes): 4.38\n",
      "\n",
      "- Taille du modele sur disque (en octets)): {'vocab': 95855000, 'vectors': 230052000, 'syn1neg': 230052000, 'total': 555959000}\n",
      "        Total en MB: 530.2\n",
      "\n",
      "- Nombre de mots encodés (= taille du vocab): 191710\n",
      "\n",
      "[('Corsa', 0.9943662285804749), ('outlying', 0.9939518570899963), ('Jamaicans', 0.9936259388923645), ('Schlegel', 0.9935262203216553), ('Orla', 0.9934742450714111), ('advancements', 0.9933976531028748), ('swabbed', 0.9933505654335022), ('excitement.', 0.9927796125411987), ('Wharrie', 0.9927717447280884), ('fides', 0.9927417039871216)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-34-50cfac195534>:61: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2v_model.init_sims(replace=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the figure\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "x and y must have same first dimension, but have shapes (0,) and (1,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-e33d39cb894e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[0mcheckpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mw2v_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtimes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msizes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mext\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtimes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msent_len\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-34-50cfac195534>\u001b[0m in \u001b[0;36mplot\u001b[1;34m(ext, times, sent_len)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent_len\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r--'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Le temps mis pour entrainer les modeles en fonction du nombre de phrases considérées\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Nombre de phrases considérées\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\pyplot.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3017\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_copy_docstring_and_deprecators\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mAxes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3018\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3019\u001b[1;33m     return gca().plot(\n\u001b[0m\u001b[0;32m   3020\u001b[0m         \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscalex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscaley\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscaley\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3021\u001b[0m         **({\"data\": data} if data is not None else {}), **kwargs)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_axes.py\u001b[0m in \u001b[0;36mplot\u001b[1;34m(self, scalex, scaley, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \"\"\"\n\u001b[0;32m   1604\u001b[0m         \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnormalize_kwargs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLine2D\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1605\u001b[1;33m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1606\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlines\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1607\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, data, *args, **kwargs)\u001b[0m\n\u001b[0;32m    313\u001b[0m                 \u001b[0mthis\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    314\u001b[0m                 \u001b[0margs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 315\u001b[1;33m             \u001b[1;32myield\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_plot_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mthis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    316\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_next_color\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\matplotlib\\axes\\_base.py\u001b[0m in \u001b[0;36m_plot_args\u001b[1;34m(self, tup, kwargs, return_kwargs)\u001b[0m\n\u001b[0;32m    499\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    500\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 501\u001b[1;33m             raise ValueError(f\"x and y must have same first dimension, but \"\n\u001b[0m\u001b[0;32m    502\u001b[0m                              f\"have shapes {x.shape} and {y.shape}\")\n\u001b[0;32m    503\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: x and y must have same first dimension, but have shapes (0,) and (1,)"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAiMAAAFpCAYAAABOASgmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8rg+JYAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQJ0lEQVR4nO3dX4jld3nH8c/TXQP1T1XMKjZ/MC3RuBem6BilaBsrrUl6EQQvEsXQICyhRrxMKFQvvKkXBRGjyxJC8MZc1KCxREOh2BRs2kxAY2KIbCNNthGyUbEQoWHN04uZyjBOMmc3Z/ZZzrxeMDC/c74z8zBfZs97f+fMb6q7AwAw5XemBwAA9jcxAgCMEiMAwCgxAgCMEiMAwCgxAgCM2jVGquqOqnqmqh55kfurqr5YVcer6uGqeufyxwQAVtUiZ0buTHLVS9x/dZJLN9+OJPnKyx8LANgvdo2R7r4/yc9fYsm1Sb7aGx5I8rqqevOyBgQAVtsyXjNyQZKnthyf2LwNAGBXB5fwOWqH23a8xnxVHcnGUzl51ate9a7LLrtsCV8eAJj20EMPPdvdh87kY5cRIyeSXLTl+MIkT++0sLuPJTmWJGtra72+vr6ELw8ATKuq/zrTj13G0zT3JLlh87dq3pvkl9390yV8XgBgH9j1zEhVfS3JlUnOr6oTST6b5BVJ0t1Hk9yb5Jokx5P8KsmNezUsALB6do2R7r5+l/s7ySeXNhEAsK+4AisAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjFoqRqrqqqh6vquNVdesO97+2qr5VVT+oqker6sbljwoArKJdY6SqDiS5LcnVSQ4nub6qDm9b9skkP+ruy5NcmeTvq+q8Jc8KAKygRc6MXJHkeHc/0d3PJ7krybXb1nSS11RVJXl1kp8nObXUSQGAlbRIjFyQ5Kktxyc2b9vqS0nenuTpJD9M8unufmEpEwIAK22RGKkdbuttxx9K8v0kv5/kj5J8qap+77c+UdWRqlqvqvWTJ0+e5qgAwCpaJEZOJLloy/GF2TgDstWNSe7uDceT/CTJZds/UXcf6+617l47dOjQmc4MAKyQRWLkwSSXVtUlmy9KvS7JPdvWPJnkg0lSVW9K8rYkTyxzUABgNR3cbUF3n6qqm5Pcl+RAkju6+9Gqumnz/qNJPpfkzqr6YTae1rmlu5/dw7kBgBWxa4wkSXffm+Tebbcd3fL+00n+YrmjAQD7gSuwAgCjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjxAgAMEqMAACjFoqRqrqqqh6vquNVdeuLrLmyqr5fVY9W1b8sd0wAYFUd3G1BVR1IcluSP09yIsmDVXVPd/9oy5rXJflykqu6+8mqeuMezQsArJhFzoxckeR4dz/R3c8nuSvJtdvWfDTJ3d39ZJJ09zPLHRMAWFWLxMgFSZ7acnxi87at3prk9VX13ap6qKpu2OkTVdWRqlqvqvWTJ0+e2cQAwEpZJEZqh9t62/HBJO9K8pdJPpTkb6vqrb/1Qd3Hunutu9cOHTp02sMCAKtn19eMZONMyEVbji9M8vQOa57t7ueSPFdV9ye5PMmPlzIlALCyFjkz8mCSS6vqkqo6L8l1Se7ZtuabSd5fVQer6pVJ3pPkseWOCgCsol3PjHT3qaq6Ocl9SQ4kuaO7H62qmzbvP9rdj1XVd5I8nOSFJLd39yN7OTgAsBqqe/vLP86OtbW1Xl9fH/naAMByVdVD3b12Jh/rCqwAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMWihGquqqqnq8qo5X1a0vse7dVfXrqvrI8kYEAFbZrjFSVQeS3Jbk6iSHk1xfVYdfZN3nk9y37CEBgNW1yJmRK5Ic7+4nuvv5JHcluXaHdZ9K8vUkzyxxPgBgxS0SIxckeWrL8YnN236jqi5I8uEkR1/qE1XVkapar6r1kydPnu6sAMAKWiRGaofbetvxF5Lc0t2/fqlP1N3Hunutu9cOHTq04IgAwCo7uMCaE0ku2nJ8YZKnt61ZS3JXVSXJ+UmuqapT3f2NZQwJAKyuRWLkwSSXVtUlSf47yXVJPrp1QXdf8v/vV9WdSf5RiAAAi9g1Rrr7VFXdnI3fkjmQ5I7ufrSqbtq8/yVfJwIA8FIWOTOS7r43yb3bbtsxQrr7r17+WADAfuEKrADAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIwSIwDAKDECAIxaKEaq6qqqeryqjlfVrTvc/7Gqenjz7XtVdfnyRwUAVtGuMVJVB5LcluTqJIeTXF9Vh7ct+0mSP+3udyT5XJJjyx4UAFhNi5wZuSLJ8e5+orufT3JXkmu3Luju73X3LzYPH0hy4XLHBABW1SIxckGSp7Ycn9i87cV8Ism3X85QAMD+cXCBNbXDbb3jwqoPZCNG3vci9x9JciRJLr744gVHBABW2SJnRk4kuWjL8YVJnt6+qKrekeT2JNd29892+kTdfay717p77dChQ2cyLwCwYhaJkQeTXFpVl1TVeUmuS3LP1gVVdXGSu5N8vLt/vPwxAYBVtevTNN19qqpuTnJfkgNJ7ujuR6vqps37jyb5TJI3JPlyVSXJqe5e27uxAYBVUd07vvxjz62trfX6+vrI1wYAlquqHjrTExGuwAoAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMAoMQIAjBIjAMCohWKkqq6qqser6nhV3brD/VVVX9y8/+GqeufyRwUAVtGuMVJVB5LcluTqJIeTXF9Vh7ctuzrJpZtvR5J8ZclzAgArapEzI1ckOd7dT3T380nuSnLttjXXJvlqb3ggyeuq6s1LnhUAWEGLxMgFSZ7acnxi87bTXQMA8FsOLrCmdritz2BNqupINp7GSZL/rapHFvj6nD3nJ3l2egh+w36ce+zJucV+nFvedqYfuEiMnEhy0ZbjC5M8fQZr0t3HkhxLkqpa7+6105qWPWVPzi3249xjT84t9uPcUlXrZ/qxizxN82CSS6vqkqo6L8l1Se7ZtuaeJDds/lbNe5P8srt/eqZDAQD7x65nRrr7VFXdnOS+JAeS3NHdj1bVTZv3H01yb5JrkhxP8qskN+7dyADAKlnkaZp0973ZCI6ttx3d8n4n+eRpfu1jp7mevWdPzi3249xjT84t9uPccsb7URsdAQAww+XgAYBRex4jLiV/bllgPz62uQ8PV9X3quryiTn3k932ZMu6d1fVr6vqI2dzvv1mkf2oqiur6vtV9WhV/cvZnnG/WeDfrddW1beq6gebe+J1i3uoqu6oqmde7PIcZ/S43t179paNF7z+Z5I/SHJekh8kObxtzTVJvp2Na5W8N8m/7+VM+/ltwf344ySv33z/avsxvydb1v1zNl679ZHpuVf1bcGfkdcl+VGSizeP3zg99yq/Lbgnf5Pk85vvH0ry8yTnTc++qm9J/iTJO5M88iL3n/bj+l6fGXEp+XPLrvvR3d/r7l9sHj6QjWvGsHcW+RlJkk8l+XqSZ87mcPvQIvvx0SR3d/eTSdLd9mRvLbInneQ1VVVJXp2NGDl1dsfcP7r7/mx8j1/MaT+u73WMuJT8ueV0v9efyEbdsnd23ZOquiDJh5McDXttkZ+RtyZ5fVV9t6oeqqobztp0+9Mie/KlJG/PxsU2f5jk0939wtkZjx2c9uP6Qr/a+zIs7VLyLMXC3+uq+kA2YuR9ezoRi+zJF5Lc0t2/3viPH3tokf04mORdST6Y5HeT/FtVPdDdP97r4fapRfbkQ0m+n+TPkvxhkn+qqn/t7v/Z49nY2Wk/ru91jCztUvIsxULf66p6R5Lbk1zd3T87S7PtV4vsyVqSuzZD5Pwk11TVqe7+xlmZcH9Z9N+sZ7v7uSTPVdX9SS5PIkb2xiJ7cmOSv+uNFywcr6qfJLksyX+cnRHZ5rQf1/f6aRqXkj+37LofVXVxkruTfNz/9M6KXfekuy/p7rd091uS/EOSvxYie2aRf7O+meT9VXWwql6Z5D1JHjvLc+4ni+zJk9k4U5WqelM2/mDbE2d1SrY67cf1PT0z0i4lf05ZcD8+k+QNSb68+T/xU+0PUe2ZBfeEs2SR/ejux6rqO0keTvJCktu7218g3yML/ox8LsmdVfXDbDxFcEt3+2u+e6SqvpbkyiTnV9WJJJ9N8orkzB/XXYEVABjlCqwAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCMEiMAwCgxAgCM+j+ba3BRzX8owAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 648x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Utilisez gensim pour entrainer des representations vectorielles sur tout ou partie du 1BWC.\n",
    "#train by tranche + save the time for each tranche\n",
    "print('Starting the training 1')\n",
    "files = listdir(data)[:nb_tranches]\n",
    "sents = []\n",
    "times = []\n",
    "times_v = []\n",
    "sizes = []\n",
    "sent_len = []\n",
    "\n",
    "# Time to build model \n",
    "start_time = time()\n",
    "w2v_model = Word2Vec(\n",
    "    sentences = sents,#LineSentence(datapath(training_sample)),\n",
    "    #corpus_file=training_sample,\n",
    "    min_count=args.mincount, window=args.window,  alpha=1e-2, vector_size=args.size,\n",
    "    min_alpha=1e-4, workers=(os.cpu_count()*2 - 1), sample=0.01, negative=args.negative\n",
    "    )\n",
    "end_m = round((time() - start_time),2)\n",
    "print(f\"- Temps de construction de modèle (en secondes): {end_m}\\n\")\n",
    "\n",
    "#for i,fn in enumerate(files):\n",
    "i,fn = 0,files[0]\n",
    "print(f\"\\n{'#-'*10}-Files 1..{i+1}:\")\n",
    "\n",
    "start_time = time()\n",
    "sents = LineSentence(data+fn)\n",
    "print(f\"- Temps de lecture de sentences (en secondes): {(time() - start_time)}\\n\")\n",
    "\n",
    "start_time = time()\n",
    "w2v_model.build_vocab(\n",
    "    corpus_iterable=sents,#LineSentence(datapath(training_sample)),\n",
    "    #corpus_file=training_sample, \n",
    "    progress_per=10000, update=True if i>0 else False\n",
    "    )\n",
    "end_v = round((time() - start_time)/60,2)\n",
    "times_v.append(end_v)\n",
    "print(f\"- Temps de construction de vocabulaire (en minutes): {end_v}\\n\")\n",
    "\n",
    "# Time to train the model\n",
    "start_time = time()\n",
    "w2v_model.train( corpus_iterable = sents,#LineSentence(datapath(training_sample)),\n",
    "    #corpus_file=training_sample,\n",
    "    total_examples=w2v_model.corpus_count, epochs=5, report_delay=1\n",
    "                ) \n",
    "end = round((time() - start_time)/60,2)\n",
    "times.append(end)\n",
    "print(f\"- Temps d'entrainement (en minutes): {end}\\n\")\n",
    "\n",
    "# Size of the model\n",
    "size = round(w2v_model.estimate_memory()['total']/(1024*1024),2)\n",
    "sizes.append(size)\n",
    "\n",
    "print(f\"- Taille du modele sur disque (en octets)): {w2v_model.estimate_memory()}\\n\\\n",
    "        Total en MB: {size}\")\n",
    "\n",
    "print(\"\\n- Nombre de mots encodés (= taille du vocab): %d\\n\" % len(w2v_model.wv.vectors))\n",
    "#print(w2v_model.most_similar(positive=['abnormalities'], topn = 10))\n",
    "#w2v_model.init_sims(replace=True)\n",
    "print(w2v_model.wv.most_similar(positive=['abnormalities'])[:10])\n",
    "\n",
    "ext = \"{}-size{}-window{}-neg{}-mincount{}\".format(args.name, args.size, args.window, args.negative, args.mincount)\n",
    "\n",
    "checkpoint(ext,save_model,w2v_model,times,sizes)\n",
    "\n",
    "plot(ext,times, sent_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'LineSentence' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-d81f36919a04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'LineSentence' has no len()"
     ]
    }
   ],
   "source": [
    "len(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x000002281427EBA0>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'LineSentence' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-47-d09da3aeb6c5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msents\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'LineSentence' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dff666bc681b740487bfa3c64f9892c436638dbe9df8fc910514714dacdf7e41"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit ('base': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
