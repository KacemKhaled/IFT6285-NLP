{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IFT6285 (TALN) \n",
    "\n",
    "## Devoir 3: Correction des mots\n",
    "\n",
    "Réalisé par:\n",
    "\n",
    "Mouna Dhaouadi \n",
    "\n",
    "Kacem Khaled\n",
    "\n",
    "Soumis à:\n",
    "\n",
    "Prof. Philippe Langlais\n",
    "\n",
    "Frédéric Piedboeuf\n",
    "\n",
    "\n",
    "**Automne 2021**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: editdistance in c:\\users\\kacem\\anaconda3\\lib\\site-packages (0.5.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: soundex in c:\\users\\kacem\\anaconda3\\lib\\site-packages (1.1.3)\n",
      "Requirement already satisfied: silpa-common>=0.3 in c:\\users\\kacem\\anaconda3\\lib\\site-packages (from soundex) (0.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install soundex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: textdistance in c:\\users\\kacem\\anaconda3\\lib\\site-packages (4.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import jaro\n",
    "import heapq as hq\n",
    "import editdistance\n",
    "#import nltk\n",
    "import soundex\n",
    "import textdistance\n",
    "from soundex import Soundex\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os.path as osp\n",
    "from os.path import dirname, abspath\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(format='%(message)s', level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = 'voc-1bwc.txt'\n",
    "wrong_words = 'devoir3-train-short.txt'\n",
    "reference = 'devoir3-train.txt'\n",
    "\n",
    "CONFIG_PATH = abspath('./')\n",
    "SRC_ROOT = dirname(CONFIG_PATH)\n",
    "VOCAB_ROOT = osp.join(SRC_ROOT, vocab)\n",
    "TRAIN_SHORT_ROOT = osp.join(SRC_ROOT, wrong_words)\n",
    "TRAIN_ROOT = osp.join(SRC_ROOT, reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 : Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_frequency_table(file_name):\n",
    "    frequency_table = {}\n",
    "    # create a dict from lexique\n",
    "    with open(file_name ,'r',encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            s = line.rstrip('\\n').lstrip()\n",
    "            frequency = int(s.split(' ')[0])\n",
    "            word = s.split(' ')[1]\n",
    "            frequency_table[word] = frequency\n",
    "    f.close()\n",
    "    return frequency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_candidates(wrong_word, vocab_scores, nb_best_scores, largest, n=5, order_by= 'unigram'): \n",
    "    scores = list(set(a[0] for a in vocab_scores.values())) \n",
    "    if largest:\n",
    "     # Find three biggest scores (sorted) --> three best possible corrections\n",
    "        best_scores= hq.nlargest(nb_best_scores, scores)\n",
    "    else:\n",
    "        best_scores=  hq.nsmallest(nb_best_scores, scores)\n",
    "            \n",
    "    best_candidates = {v:vocab_scores[v] for v in vocab_scores if vocab_scores[v][0] in best_scores}\n",
    "    \n",
    "    if order_by == 'distance':\n",
    "        best_candidates = sorted(best_candidates, key=lambda k: (best_candidates[k][0]), reverse=largest)\n",
    "        #print(best_candidates[:30])\n",
    "        corrections = [c for c in best_candidates][:n]\n",
    "    elif order_by == 'unigram':\n",
    "        best_candidates = sorted(best_candidates, key=lambda k: (best_candidates[k][1]), reverse=largest)\n",
    "        #print(best_candidates[:30])\n",
    "        corrections = [c for c in best_candidates][:n]\n",
    "    elif order_by == 'comb_d_u':\n",
    "        best_candidates = sorted(best_candidates, key=lambda k: (best_candidates[k][0],best_candidates[k][1]), reverse=largest)\n",
    "        #print(best_candidates[:30])\n",
    "        corrections = [c for c in best_candidates][:n]\n",
    "    elif order_by == 'comb_u_d':\n",
    "        best_candidates = sorted(best_candidates, key=lambda k: (best_candidates[k][1],best_candidates[k][0]), reverse=largest)\n",
    "        #print(best_candidates[:30])\n",
    "        corrections = [c for c in best_candidates][:n]\n",
    "\n",
    "    affichage(wrong_word, corrections)\n",
    "    \n",
    "    return corrections, best_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affichage(wrong_word, corrections):\n",
    "    if len(corrections) == 0 : \n",
    "        result =  wrong_word + '\\t' + wrong_word\n",
    "    else:\n",
    "        result =  wrong_word + '\\t' + '\\t'.join(corrections)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(dispatcher,distance_name,wrong_word,frequency_table,vocab,ordre):\n",
    "    \n",
    "    if distance_name==\"Soundex\":\n",
    "        instance = Soundex()\n",
    "        # We want Englidh corrections --> remove non english corrections (-1)\n",
    "        vocab = [ v for v in vocab if instance.compare(v, wrong_word) >= 0 ]\n",
    "    distance_scores = { v: [dispatcher[distance_name][0](v, wrong_word), frequency_table[v]] for v in vocab }\n",
    "    logging.info(f'Corrections using {distance_name} distance :')\n",
    "    find_best_candidates(wrong_word, distance_scores, 3, dispatcher[distance_name][1],5, order_by= ordre) # 'distance' , 'unigram' or 'comb_d_u' or 'comb_u_d'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrige(wrong_words, lexique_file, distances=['Jaro_Winkler','Levenshtein'], order_by='unigram'):\n",
    "\n",
    "    frequency_table = get_frequency_table(lexique_file)\n",
    "    vocab = list(frequency_table.keys())\n",
    "    instance = Soundex()\n",
    "\n",
    "    # https://pypi.org/project/textdistance/\n",
    "    dispatcher = {\n",
    "        'Jaro_Winkler' : [textdistance.jaro_winkler.distance,False],\n",
    "        'Levenshtein' : [textdistance.levenshtein.distance,False],\n",
    "        'Jaccard': [textdistance.jaccard.distance,False],\n",
    "        'Cosine': [textdistance.cosine.distance,False],\n",
    "        'Hamming': [textdistance.hamming.distance,False],\n",
    "        'LCSS': [textdistance.lcsstr.distance,False], # textdistance.lcsstr.similarity,True\n",
    "        'Damerau_Levenshtein': [textdistance.damerau_levenshtein.distance,False],\n",
    "        'Needleman_Wunsch': [textdistance.needleman_wunsch.distance,False],\n",
    "        'Soundex': [instance.compare,False]\n",
    "                 }\n",
    "    \n",
    "\n",
    "    # lit une liste de mots a corriger, un par ligne\n",
    "    with open(wrong_words, 'r',encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "\n",
    "            wrong_word = line.rstrip().split()[0]\n",
    "            logging.info('********************************* ' + wrong_word)\n",
    "            # calculate the distance for all the vocabulary\n",
    "            # for each word in vocabulary, create a dictionary of candidates with 'nb_best_scores': {cand1:score1,cand2:score2,...}\n",
    "            # get the 'n' best candidates and order them according to a criteria: 'distance' , 'unigram' or 'comb_d_u' or 'comb_u_d'\n",
    "            \n",
    "            for d in distances:\n",
    "                process(dispatcher,d,wrong_word,frequency_table,vocab,order_by)\n",
    "\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "********************************* reccodmission\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrections using Jaro_Winkler distance :\n",
      "reccodmission\trecommissioned\tcomission\tre-admission\n",
      "Corrections using Levenshtein distance :\n",
      "reccodmission\tre-admission\tdecommission\treadmission\tre-admissions\trecission\n",
      "Corrections using Jaccard distance :\n",
      "reccodmission\tmicroseconds\tmicrosecond\trecommissioned\tsemiconductors\n",
      "Corrections using Cosine distance :\n",
      "reccodmission\tmicroseconds\tmicrosecond\trecommissioned\tsemiconductors\n",
      "Corrections using Hamming distance :\n",
      "reccodmission\tzero-emission\tco-commissioned\tenviromission\thigh-emission\tmulti-mission\n",
      "Corrections using LCSS distance :\n",
      "reccodmission\tre-admissions\tre-admission\treadmissions\treadmission\tadmissions\n",
      "Corrections using Damerau_Levenshtein distance :\n",
      "reccodmission\tre-admission\tdecommission\treadmission\tre-admissions\trecission\n",
      "Corrections using Needleman_Wunsch distance :\n",
      "reccodmission\tre-admission\tdecommission\tre-admissions\trecommissioned\tresubmission\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "********************************* unpair\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrections using Soundex distance :\n",
      "reccodmission\tcost-to-income\ttostenson\tboston.com\thosting.com\tdistension\n",
      "Corrections using Jaro_Winkler distance :\n",
      "unpair\tunpaid\tunfair\tunai\n",
      "Corrections using Levenshtein distance :\n",
      "unpair\tunpaid\tunfair\tusair\tin-air\tumair\n",
      "Corrections using Jaccard distance :\n",
      "unpair\tpurina\tnirupam\tpianura\tmanipur\tpuritan\n",
      "Corrections using Cosine distance :\n",
      "unpair\tpurina\tnirupam\tpianura\tmanipur\tpuritan\n",
      "Corrections using Hamming distance :\n",
      "unpair\tunpaid\tunfair\tin-air\tunsaid\tunpack\n",
      "Corrections using LCSS distance :\n",
      "unpair\tunpaid\tpairc\tunpack\timpair\tpaired\n",
      "Corrections using Damerau_Levenshtein distance :\n",
      "unpair\tunpaid\tunfair\tusair\tin-air\tumair\n",
      "Corrections using Needleman_Wunsch distance :\n",
      "unpair\tunpaid\tunfair\tin-air\tfunfair\tunsaid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "********************************* oconomic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrections using Soundex distance :\n",
      "unpair\tcombivir\tdenber\tembrey\ten-pro\tinfraero\n",
      "Corrections using Jaro_Winkler distance :\n",
      "oconomic\toconomowoc\toncologic\tcnmi\toconee\tzoonotic\n",
      "Corrections using Levenshtein distance :\n",
      "oconomic\teconomic\teconomico\teconomix\teconomics\tbonomi\n",
      "Corrections using Jaccard distance :\n",
      "oconomic\teconomico\tcominco\tcoconino\teconomic\n",
      "Corrections using Cosine distance :\n",
      "oconomic\teconomico\tcominco\tcoconino\teconomic\n",
      "Corrections using Hamming distance :\n",
      "oconomic\teconomic\teconomico\teconomix\teconomics\teconomicus\n",
      "Corrections using LCSS distance :\n",
      "oconomic\teconomic\teconomico\teconomix\teconomics\tbonomi\n",
      "Corrections using Damerau_Levenshtein distance :\n",
      "oconomic\teconomic\teconomico\teconomix\teconomics\tbonomi\n",
      "Corrections using Needleman_Wunsch distance :\n",
      "oconomic\teconomic\teconomico\teconomix\teconomics\teconomicus\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "********************************* accidnts\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrections using Soundex distance :\n",
      "oconomic\tagenc\tausenco\tbushisms\tccms\tcieszynski\n",
      "Corrections using Jaro_Winkler distance :\n",
      "accidnts\taccidents\taccident\taccidently\taccidental\n",
      "Corrections using Levenshtein distance :\n",
      "accidnts\taccidents\tancients\taccents\taccident\taccounts\n",
      "Corrections using Jaccard distance :\n",
      "accidnts\taccidents\tsandcity\tsantucci\tantacids\tdynastic\n",
      "Corrections using Cosine distance :\n",
      "accidnts\taccidents\tsandcity\tsantucci\tantacids\tdynastic\n",
      "Corrections using Hamming distance :\n",
      "accidnts\tancients\taccounts\taccion\tacciona\tancient\n",
      "Corrections using LCSS distance :\n",
      "accidnts\tflaccid\taccident\tbacci\tmasaccio\tceccacci\n",
      "Corrections using Damerau_Levenshtein distance :\n",
      "accidnts\taccidents\tancients\taccents\taccident\taccounts\n",
      "Corrections using Needleman_Wunsch distance :\n",
      "accidnts\taccidents\tancients\taccounts\taccredits\taccidently\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "********************************* succes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrections using Soundex distance :\n",
      "accidnts\toxidants\teight-months\tmis-statements\tnews-stands\tsystematics\n",
      "Corrections using Jaro_Winkler distance :\n",
      "succes\tsucces\tsuccess\tsucceeds\n",
      "Corrections using Levenshtein distance :\n",
      "succes\tsucces\tsuccess\tsuceed\tpuces\tsucher\n",
      "Corrections using Jaccard distance :\n",
      "succes\tsucces\taccuses\tsuccess\tscuse\n",
      "Corrections using Cosine distance :\n",
      "succes\tsucces\taccuses\tsuccess\tscuse\n",
      "Corrections using Hamming distance :\n",
      "succes\tsucces\tsuccess\tsuceed\tsucher\tsucess\n",
      "Corrections using LCSS distance :\n",
      "succes\tsucces\tsuccess\tsuccor\tsuccop\tsucceed\n",
      "Corrections using Damerau_Levenshtein distance :\n",
      "succes\tsucces\tsuccess\tsuceed\tpuces\tsucher\n",
      "Corrections using Needleman_Wunsch distance :\n",
      "succes\tsucces\tsuccess\tsuceed\tsucher\tsluices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "********************************* enthousiastic\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrections using Soundex distance :\n",
      "succes\tsucces\tacass\tacsa\taezs\tage-y\n",
      "Corrections using Jaro_Winkler distance :\n",
      "enthousiastic\tenthusiastic\tenthusiast\tenthusiasts\n",
      "Corrections using Levenshtein distance :\n",
      "enthousiastic\tenthusiastic\tunenthusiastic\tenthusiast\tenthusiasts\tenthusiasms\n",
      "Corrections using Jaccard distance :\n",
      "enthousiastic\tenthusiastic\tunsophisticated\teducationists\tresuscitation\n",
      "Corrections using Cosine distance :\n",
      "enthousiastic\tenthusiastic\tunsophisticated\teducationists\tresuscitation\n",
      "Corrections using Hamming distance :\n",
      "enthousiastic\teuphorbias\tanti-climatic\tantiballistic\tanticlimactic\tintergalactic\n",
      "Corrections using LCSS distance :\n",
      "enthousiastic\tenthusiastic\tpenthouses\tunenthusiastic\tpenthouse\teventhough\n",
      "Corrections using Damerau_Levenshtein distance :\n",
      "enthousiastic\tenthusiastic\tunenthusiastic\tenthusiast\tenthusiasts\tenthusiasms\n",
      "Corrections using Needleman_Wunsch distance :\n",
      "enthousiastic\tenthusiastic\tunenthusiastic\tenthusiasts\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "********************************* produse\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrections using Soundex distance :\n",
      "enthousiastic\tanti-static\tamethysts\tventastega\tnon-autistic\tanti-scottish\n",
      "Corrections using Jaro_Winkler distance :\n",
      "produse\tproduces\tprodu\tprods\tproduce\tprofuse\n",
      "Corrections using Levenshtein distance :\n",
      "produse\tprofuse\tproduce\tprodisc\tsprouse\tprodu\n",
      "Corrections using Jaccard distance :\n",
      "produse\tpurposed\tpostured\tpounders\tproudest\tsprouted\n",
      "Corrections using Cosine distance :\n",
      "produse\tpurposed\tpostured\tpounders\tproudest\tsprouted\n",
      "Corrections using Hamming distance :\n",
      "produse\tprofuse\tproduce\tprodisc\tprodu\tpromus\n",
      "Corrections using LCSS distance :\n",
      "produse\tprodu\tproduce\tproduct\tprodhan\tcarrodus\n",
      "Corrections using Damerau_Levenshtein distance :\n",
      "produse\tprofuse\tproduce\tprodisc\tsprouse\tprodu\n",
      "Corrections using Needleman_Wunsch distance :\n",
      "produse\tprofuse\tproduce\tprodisc\tprocure\tpropose\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "********************************* tecnologycal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrections using Soundex distance :\n",
      "produse\tartuso\tbarrettes\tbirdcages\tbredius\tbrodies\n",
      "Corrections using Jaro_Winkler distance :\n",
      "tecnologycal\ttechnological\ttechnology\ttechnologically\n",
      "Corrections using Levenshtein distance :\n",
      "tecnologycal\ttechnological\tteleological\tecological\tethnological\tselenological\n",
      "Corrections using Jaccard distance :\n",
      "tecnologycal\ttechnologically\tgynecological\ttechnological\tclean-technology\tcolangelo\n",
      "Corrections using Cosine distance :\n",
      "tecnologycal\ttechnologically\tgynecological\ttechnological\tclean-technology\n",
      "Corrections using Hamming distance :\n",
      "tecnologycal\tteleological\tethnological\tphonological\tsociological\tneurological\n",
      "Corrections using LCSS distance :\n",
      "tecnologycal\tvaccinology\toenology\ttechnology-\tsynology\tenology\n",
      "Corrections using Damerau_Levenshtein distance :\n",
      "tecnologycal\ttechnological\tethnological\tteleological\tecological\tselenological\n",
      "Corrections using Needleman_Wunsch distance :\n",
      "tecnologycal\ttechnological\tteleological\tethnological\tselenological\tphonological\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "********************************* upto\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrections using Soundex distance :\n",
      "tecnologycal\tcosmological\tseismological\ttechnologically\ttechnological\taaaa\n",
      "Corrections using Jaro_Winkler distance :\n",
      "upto\tupto\tupton\tuspto\n",
      "Corrections using Levenshtein distance :\n",
      "upto\tupto\tupdo\tpto\tuspto\tunto\n",
      "Corrections using Jaccard distance :\n",
      "upto\toput\tupto\tpout\tstoup\ttoups\n",
      "Corrections using Cosine distance :\n",
      "upto\toput\tupto\tpout\tstoup\ttoups\n",
      "Corrections using Hamming distance :\n",
      "upto\tupto\tupdo\tunto\tupton\tanto\n",
      "Corrections using LCSS distance :\n",
      "upto\tupto\tptoi\tpto\tsupt\tupton\n",
      "Corrections using Damerau_Levenshtein distance :\n",
      "upto\tupto\tupdo\tpto\tuspto\tunto\n",
      "Corrections using Needleman_Wunsch distance :\n",
      "upto\tupto\tupdo\tuspto\tunto\tupton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "********************************* poisioning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrections using Soundex distance :\n",
      "upto\tupto\tabotu\tabutu\tbiffed\tcavitt\n",
      "Corrections using Jaro_Winkler distance :\n",
      "poisioning\tpoisoning\tprovisioning\tpositioning\n",
      "Corrections using Levenshtein distance :\n",
      "poisioning\tpoisoning\tpoisonings\tprovisioning\tpositioning\tco-signing\n",
      "Corrections using Jaccard distance :\n",
      "poisioning\tpositioning\tpoisoning\tprovisioning\n",
      "Corrections using Cosine distance :\n",
      "poisioning\tpositioning\tpoisoning\tprovisioning\n",
      "Corrections using Hamming distance :\n",
      "poisioning\tco-signing\tmoistening\tconsigning\tpostponing\troistering\n",
      "Corrections using LCSS distance :\n",
      "poisioning\tdecisioning\tenvisioning\toptioning\tsectioning\tsuctioning\n",
      "Corrections using Damerau_Levenshtein distance :\n",
      "poisioning\tpoisoning\tpoisonings\tprovisioning\tpositioning\tco-signing\n",
      "Corrections using Needleman_Wunsch distance :\n",
      "poisioning\tprovisioning\tpositioning\tpoisoning\tco-signing\tmoistening\n",
      "Corrections using Soundex distance :\n",
      "poisioning\tagenc\tausenco\tbushisms\tccms\tcieszynski\n"
     ]
    }
   ],
   "source": [
    "# distances = ['Jaro_Winkler', 'Levenshtein', 'Jaccard', 'Cosine', 'Hamming', 'LCSS', 'Damerau_Levenshtein', 'Needleman_Wunsch', 'Soundex']\n",
    "distances = ['Jaro_Winkler', 'Levenshtein', 'Jaccard', 'Cosine', 'Hamming', 'LCSS', 'Damerau_Levenshtein', 'Needleman_Wunsch', 'Soundex']\n",
    "corrige(wrong_words, lexique,distances, order_by='distance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdistance.cosine.distance('test', 'text') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre programme prendra comme seul argument obligatoire le nom d'un lexique de mots \n",
    "# Vous pouvez doter votre programme d'options permettant de contr^oler le comportement de votre correcteur. \n",
    "\n",
    "# TODO a seperate python script that we can run with the provided command in TP\n",
    "distances = ['Jaro_Winkler', 'Levenshtein', 'Jaccard', 'Cosine', 'Hamming', 'LCSS', 'Damerau_Levenshtein', 'Needleman_Wunsch', 'Soundex']\n",
    "order_by = ['distance','unigram','comb_d_u','comb_u_d']\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Words correction')\n",
    "parser.add_argument('-v','--vocab', default=VOCAB_ROOT,\n",
    "                    help='Path to vocab file: (default: ./voc-1bwc.txt)')\n",
    "\n",
    "\n",
    "parser.add_argument('-w','--wrong_words', default=TRAIN_SHORT_ROOT,\n",
    "                    help='Path to wrong words file: (default: ./devoir3-train-short.txt)')\n",
    "\n",
    "parser.add_argument('-d', '--distance', nargs='+', default=['Jaro_Winkler'],\n",
    "                    choices=distances,\n",
    "                    help='Distances: ' +\n",
    "                        ' | '.join(distances) +\n",
    "                        ' (default: Jaro_Winkler)')\n",
    "\n",
    "parser.add_argument('-o', '--order', default='unigram',\n",
    "                    choices=order_by,\n",
    "                    help='Order by: ' +\n",
    "                        ' | '.join(order_by) +\n",
    "                        ' (default: unigram)')\n",
    "\n",
    "\n",
    "def main():\n",
    "    args = parser.parse_args()\n",
    "    vocab = osp.join(SRC_ROOT, args.vocab)\n",
    "    wrong_words = osp.join(SRC_ROOT, args.wrong_words)\n",
    "    corrige(wrong_words, vocab, args.distance, args.order)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "\n",
    "# python corrige.py -v voc-1bwc.txt -w devoir3-train-short.txt -d Jaro_Winkler -o unigram > devoir3-sortie.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "********************************* reccodmission\n",
      "Corrections using Jaro_Winkler distance :\n",
      "********************************* unpair\n",
      "Corrections using Jaro_Winkler distance :\n",
      "********************************* oconomic\n",
      "Corrections using Jaro_Winkler distance :\n",
      "********************************* accidnts\n",
      "Corrections using Jaro_Winkler distance :\n",
      "********************************* succes\n",
      "Corrections using Jaro_Winkler distance :\n",
      "********************************* enthousiastic\n",
      "Corrections using Jaro_Winkler distance :\n",
      "********************************* produse\n",
      "Corrections using Jaro_Winkler distance :\n",
      "********************************* tecnologycal\n",
      "Corrections using Jaro_Winkler distance :\n",
      "********************************* upto\n",
      "Corrections using Jaro_Winkler distance :\n",
      "********************************* poisioning\n",
      "Corrections using Jaro_Winkler distance :\n"
     ]
    }
   ],
   "source": [
    "!python corrige.py -v voc-1bwc.txt -w devoir3-train-short.txt -d Jaro_Winkler -o unigram > devoir3-sortie.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = 'devoir3-train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(correction_line, reference):\n",
    "    # look for wrong word in the reference file\n",
    "    \n",
    "    # take the correct correctionss\n",
    "    \n",
    "    # compute mesure de qualite ? \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{i:i for i in range(10)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance = Soundex()\n",
    "instance.compare('cost-to-income','reccodmission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance_name ='Soundex'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_top_scores= 3 if distance_name != 'Soundex' else 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_top_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
