{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IFT6285 (TALN) \n",
    "\n",
    "## Devoir 3: Correction des mots\n",
    "\n",
    "Réalisé par:\n",
    "\n",
    "Mouna Dhaouadi \n",
    "\n",
    "Kacem Khaled\n",
    "\n",
    "Soumis à:\n",
    "\n",
    "Prof. Philippe Langlais\n",
    "\n",
    "Frédéric Piedboeuf\n",
    "\n",
    "\n",
    "**Automne 2021**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install editdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install soundex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jaro\n",
    "import heapq as hq\n",
    "import editdistance\n",
    "import nltk\n",
    "import soundex\n",
    "import textdistance\n",
    "from soundex import Soundex\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lexique = 'voc-1bwc.txt'\n",
    "wrong_words = 'test-mouna.txt'\n",
    "reference = 'devoir3-train.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1 : Correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def get_frequency_table(file_name):\n",
    "    frequency_table = {}\n",
    "    # create a dict from lexique\n",
    "    with open(file_name ,'r',encoding=\"utf8\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            s = line.rstrip('\\n').lstrip()\n",
    "            frequency = int(s.split(' ')[0])\n",
    "            word = s.split(' ')[1]\n",
    "            frequency_table[word] = frequency\n",
    "    f.close()\n",
    "    return frequency_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_candidates(wrong_word, vocab_scores, nb_best_scores, largest, n=5, ordre= 'distance'): \n",
    "    scores = list(set(a[0] for a in vocab_scores.values())) \n",
    "    if largest:\n",
    "     # Find three biggest scores (sorted) --> three best possible corrections\n",
    "        best_scores= hq.nlargest(nb_best_scores, scores)\n",
    "    else:\n",
    "        best_scores=  hq.nsmallest(nb_best_scores, scores)\n",
    "            \n",
    "    best_candidates = {v:vocab_scores[v] for v in vocab_scores if vocab_scores[v][0] in best_scores}\n",
    "    \n",
    "    if ordre == 'distance':\n",
    "        best_candidates = sorted(best_candidates, key=lambda k: (best_candidates[k][0]), reverse=largest)\n",
    "        #print(best_candidates[:30])\n",
    "        corrections = [c for c in best_candidates][:n]\n",
    "    elif ordre == 'unigram':\n",
    "        best_candidates = sorted(best_candidates, key=lambda k: (best_candidates[k][1]), reverse=largest)\n",
    "        #print(best_candidates[:30])\n",
    "        corrections = [c for c in best_candidates][:n]\n",
    "    elif ordre == 'comb_d_u':\n",
    "        best_candidates = sorted(best_candidates, key=lambda k: (best_candidates[k][0],best_candidates[k][1]), reverse=largest)\n",
    "        #print(best_candidates[:30])\n",
    "        corrections = [c for c in best_candidates][:n]\n",
    "    elif ordre == 'comb_u_d':\n",
    "        best_candidates = sorted(best_candidates, key=lambda k: (best_candidates[k][1],best_candidates[k][0]), reverse=largest)\n",
    "        #print(best_candidates[:30])\n",
    "        corrections = [c for c in best_candidates][:n]\n",
    "\n",
    "\n",
    "    affichage(wrong_word, corrections)\n",
    "    \n",
    "    return corrections, best_candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affichage(wrong_word, corrections):\n",
    "    if len(corrections) == 0 : \n",
    "        result =  wrong_word + '\\t' + wrong_word\n",
    "    else:\n",
    "        result =  wrong_word + '\\t' + '\\t'.join(corrections)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process(dispatcher,distance_name,wrong_word,frequency_table,vocab):\n",
    "    if distance_name==\"Soundex\":\n",
    "        # We want Englidh corrections --> remove non english corrections (-1)\n",
    "        vocab = [ v for v in vocab if instance.compare(v, wrong_word) >= 0 ]\n",
    "    distance_scores = { v: [dispatcher[distance_name](v, wrong_word), frequency_table[v]] for v in vocab }\n",
    "    print(f'Corrections using {distance_name} distance :')\n",
    "    jaro_winkler_corrections,_ = find_best_candidates(wrong_word, distance_scores, 3, False,5, ordre= 'unigram') # 'distance' , 'unigram' or 'comb_d_u' or 'comb_u_d'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrige(wrong_words, lexique_file, distances=['Jaro_Winkler','Levenshtein']):\n",
    "\n",
    "    frequency_table = get_frequency_table(lexique_file)\n",
    "    vocab = list(frequency_table.keys())\n",
    "    instance = Soundex()\n",
    "\n",
    "    # https://pypi.org/project/textdistance/\n",
    "    dispatcher = {\n",
    "        'Jaro_Winkler' : textdistance.jaro_winkler.distance,\n",
    "        'Levenshtein' : textdistance.levenshtein.distance,\n",
    "        'Jaccard': textdistance.jaccard.distance,\n",
    "        'Cosine': textdistance.cosine.distance,\n",
    "        'Hamming': textdistance.hamming.distance,\n",
    "        'LCSS': textdistance.lcsstr.distance,\n",
    "        'Damerau_Levenshtein': textdistance.damerau_levenshtein.distance,\n",
    "        'Needleman_Wunsch': textdistance.needleman_wunsch.distance,\n",
    "        'Soundex': instance.compare\n",
    "                 }\n",
    "    \n",
    "\n",
    "    # lit une liste de mots a corriger, un par ligne\n",
    "    with open(wrong_words, 'r',encoding=\"utf8\") as f:\n",
    "        for line in f:\n",
    "\n",
    "            wrong_word = line.rstrip().split()[0]\n",
    "            print('********************************* ' + wrong_word)\n",
    "            # calculate the distance for all the vocabulary\n",
    "            # for each word in vocabulary, create a dictionary of candidates with 'nb_best_scores': {cand1:score1,cand2:score2,...}\n",
    "            # get the 'n' best candidates and order them according to a criteria: 'distance' , 'unigram' or 'comb_d_u' or 'comb_u_d'\n",
    "            \n",
    "            for d in distances:\n",
    "                process(dispatcher,d,wrong_word,frequency_table,vocab)\n",
    "\n",
    "    f.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********************************* aple\n",
      "Corrections using Jaro_Winkler distance :\n",
      "aple\tapley\taple\tapple\n",
      "Corrections using Levenshtein distance :\n",
      "aple\tacle\tapplet\taslo\thplc\tkable\n",
      "Corrections using Jaccard distance :\n",
      "aple\tapley\tpalea\talpen\tapeal\tapfel\n",
      "Corrections using Cosine distance :\n",
      "aple\tapley\tpalea\talpen\tapeal\tapfel\n",
      "Corrections using Hamming distance :\n",
      "aple\tacle\taslo\thplc\todle\tvpls\n",
      "Corrections using LCSS distance :\n",
      "aple\tacle\terap\thplc\tkapi\tleet\n",
      "Corrections using Damerau_Levenshtein distance :\n",
      "aple\tacle\tapplet\taslo\thplc\tkable\n",
      "Corrections using Needleman_Wunsch distance :\n",
      "aple\tacle\tapplet\taslo\thplc\tkable\n",
      "Corrections using Soundex distance :\n",
      "aple\taaaa\taaa-rating\taaoifi\taarti\tabaca\n",
      "********************************* juce\n",
      "Corrections using Jaro_Winkler distance :\n",
      "juce\tjuche\tjuckes\tjuice\n",
      "Corrections using Levenshtein distance :\n",
      "juce\tacle\tauh\tbbcs\tboice\tbuker\n",
      "Corrections using Jaccard distance :\n",
      "juce\tuce\tujc\tjue\tceu\tjudice\n",
      "Corrections using Cosine distance :\n",
      "juce\tuce\tujc\tjue\tceu\tjudice\n",
      "Corrections using Hamming distance :\n",
      "juce\tacle\tauh\tbbcs\tbuker\tbyc\n",
      "Corrections using LCSS distance :\n",
      "juce\tacle\tacsa\tadmc\taers\taetn\n",
      "Corrections using Damerau_Levenshtein distance :\n",
      "juce\tacle\tauh\tbbcs\tboice\tbuker\n",
      "Corrections using Needleman_Wunsch distance :\n",
      "juce\tacle\tbbcs\tboice\tbuker\tccie\n",
      "Corrections using Soundex distance :\n",
      "juce\taaaa\taaa-rating\taaoifi\taarti\tabaca\n",
      "********************************* freze\n",
      "Corrections using Jaro_Winkler distance :\n",
      "freze\tfrenzel\tfrieze\tfree\n",
      "Corrections using Levenshtein distance :\n",
      "freze\taezs\tardee\tbeese\tbrehme\tbrise\n",
      "Corrections using Jaccard distance :\n",
      "freze\tferoze\tfrazee\tfeer\tfetzer\terez\n",
      "Corrections using Cosine distance :\n",
      "freze\tferoze\tfrazee\tfeer\tfetzer\terez\n",
      "Corrections using Hamming distance :\n",
      "freze\tardee\tbeese\tbrise\tdref\tdresen\n",
      "Corrections using LCSS distance :\n",
      "freze\taarti\tabron\tacle\tadwar\taers\n",
      "Corrections using Damerau_Levenshtein distance :\n",
      "freze\taezs\tardee\tbeese\tbrehme\tbrise\n",
      "Corrections using Needleman_Wunsch distance :\n",
      "freze\tardee\tbeese\tbrehme\tbrise\tdresen\n",
      "Corrections using Soundex distance :\n",
      "freze\taaaa\taaa-rating\taaoifi\taarti\tabaca\n"
     ]
    }
   ],
   "source": [
    "# distances = ['Jaro_Winkler', 'Levenshtein', 'Jaccard', 'Cosine', 'Hamming', 'LCSS', 'Damerau_Levenshtein', 'Needleman_Wunsch', 'Soundex']\n",
    "distances = ['Jaro_Winkler', 'Levenshtein', 'Jaccard', 'Cosine', 'Hamming', 'LCSS', 'Damerau_Levenshtein', 'Needleman_Wunsch', 'Soundex']\n",
    "corrige(wrong_words, lexique,distances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.25"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textdistance.cosine.distance('test', 'text') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Votre programme prendra comme seul argument obligatoire le nom d'un lexique de mots \n",
    "# Vous pouvez doter votre programme d'options permettant de contr^oler le comportement de votre correcteur. \n",
    "\n",
    "# TODO a seperate python script that we can run with the provided command in TP\n",
    "import argparse\n",
    "\n",
    "order_by = ['distance','unigram','comb_d_u','comb_u_d']\n",
    "\n",
    "parser = argparse.ArgumentParser(description='Words correction')\n",
    "parser.add_argument('-d', '--distance', default='Jaro_Winkler',\n",
    "                    choices=distances,\n",
    "                    help='Distances: ' +\n",
    "                        ' | '.join(distances) +\n",
    "                        ' (default: Jaro_Winkler)')\n",
    "\n",
    "parser.add_argument('-o', '--order', default='unigram',\n",
    "                    choices=order_by,\n",
    "                    help='Distances: ' +\n",
    "                        ' | '.join(order_by) +\n",
    "                        ' (default: unigram)')\n",
    "\n",
    "def main():\n",
    "    args = parser.parse_args()\n",
    "    #args.distance\n",
    "    #args.order\n",
    "    corrige(reference, lexique,args.distance)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrige(reference, lexique,['Jaro_Winkler'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference = 'devoir3-train.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(correction_line, reference):\n",
    "    # look for wrong word in the reference file\n",
    "    \n",
    "    # take the correct correctionss\n",
    "    \n",
    "    # compute mesure de qualite ? \n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
